import json
import os
import sys
from pathlib import Path
import time
from typing import Any, Dict, List, Optional, Tuple, Union
from imutils.paths import list_images
from keras_unet_collection.losses import focal_tversky, iou_seg
import numpy as np
import cv2
import xml.etree.ElementTree as ET
import glob
import pandas as pd
import albumentations as A
from pycocotools.coco import COCO

import streamlit as st
from streamlit import session_state
from stqdm import stqdm

from object_detection.utils import label_map_util

# >>>>>>>>>>>>>>>>>>>>>>TEMP>>>>>>>>>>>>>>>>>>>>>>>>

SRC = Path(__file__).resolve().parents[2]  # ROOT folder -> ./src
LIB_PATH = SRC / "lib"

if str(LIB_PATH) not in sys.path:
    sys.path.insert(0, str(LIB_PATH))  # ./lib
else:
    pass

# >>>> User-defined Modules >>>>
from core.utils.log import logger


def load_image_into_numpy_array(path: str):
    """Load an image from file into a numpy array.
    Puts image into numpy array of shape (height, width, channels), where channels=3 for RGB to feed into tensorflow graph.
    Args:
    path: the file path to the image
    Returns:
    uint8 numpy array with shape (img_height, img_width, 3)
    """
    # always read in 3 channels
    img = cv2.imread(path, cv2.IMREAD_COLOR)
    # convert from OpenCV's BGR to RGB format
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    return img


def get_transform():
    """Get the Albumentations' transform using the existing augmentation config stored in DB."""
    existing_aug = session_state.new_training.augmentation_config.augmentations

    transform_list = []
    for transform_name, param_values in existing_aug.items():
        transform_list.append(getattr(A, transform_name)(**param_values))

    if session_state.project.deployment_type == 'Object Detection with Bounding Boxes':
        min_area = session_state.new_training.augmentation_config.min_area
        min_visibility = session_state.new_training.augmentation_config.min_visibility
        transform = A.Compose(
            transform_list,
            bbox_params=A.BboxParams(
                format='pascal_voc',
                min_area=min_area,
                min_visibility=min_visibility,
                label_fields=['class_names']
            ))
    else:
        transform = A.Compose(transform_list)
    return transform

# ******************************* TFOD funcs *******************************


@st.experimental_memo
def xml_to_df(path: str) -> pd.DataFrame:
    """
    If a path to XML file is passed in, parse it directly.
    If directory is passed in, iterates through all .xml files (generated by our custom Label Studio) 
    in a given directory and combines them in a single Pandas dataframe.
    NOTE: This function will not work for the original Label Studio, because they export
    Pascal VOC XML files without the image extensions in the <filename> tags.

    Parameters:
    ----------
    path : str
        The path containing the .xml files
    Returns
    -------
    Pandas DataFrame
        The produced dataframe
    """
    if isinstance(path, Path):
        path = str(path)

    xml_list = []

    if os.path.isfile(path):
        xml_files = [path]
    else:
        xml_files = glob.glob(path + "/*.xml")

    for xml_file in xml_files:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        filename = root.find("filename").text
        width = int(root.find("size").find("width").text)
        height = int(root.find("size").find("height").text)
        for member in root.findall("object"):
            bndbox = member.find("bndbox")
            value = (
                filename,
                width,
                height,
                member.find("name").text,
                int(bndbox.find("xmin").text),
                int(bndbox.find("ymin").text),
                int(bndbox.find("xmax").text),
                int(bndbox.find("ymax").text),
            )
            xml_list.append(value)
    column_name = [
        "filename",
        "width",
        "height",
        "classname",
        "xmin",
        "ymin",
        "xmax",
        "ymax",
    ]
    xml_df = pd.DataFrame(xml_list, columns=column_name)
    return xml_df


def get_bbox_label_info(xml_df: pd.DataFrame,
                        image_name: str) -> Tuple[List[str], Tuple[int, int, int, int]]:
    """Get the class name and bounding box coordinates associated with the image."""
    annot_df = xml_df.loc[xml_df['filename'] == image_name]
    class_names = annot_df['classname'].values
    bboxes = annot_df.loc[:, 'xmin': 'ymax'].values
    return class_names, bboxes


def generate_tfod_xml_csv(image_paths: List[str],
                          xml_dir: Path,
                          output_img_dir: Path,
                          csv_path: Path,
                          train_size: int):
    """Generate TFOD's CSV file for augmented images and bounding boxes used for generating TF Records.
    Also save the transformed images to the `output_img_dir` at the same time."""

    output_img_dir.mkdir(parents=True, exist_ok=True)

    transform = get_transform()
    xml_df = xml_to_df(str(xml_dir))

    if train_size > len(image_paths):
        # randomly select the remaining paths and extend them to the original List
        # to make sure to go through the entire dataset for at least once
        n_remaining = train_size - len(image_paths)
        image_paths.extend(np.random.choice(
            image_paths, size=n_remaining, replace=True))

    logger.info('Generating CSV file for augmented bounding boxes ...')
    start = time.perf_counter()
    xml_list = []
    for image_path in stqdm(image_paths):
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        filename = os.path.basename(image_path)
        class_names, bboxes = get_bbox_label_info(xml_df, filename)
        width, height = xml_df.loc[xml_df['filename'] == filename,
                                   'width': 'height'].values[0]

        transformed = transform(image=image, bboxes=bboxes,
                                class_names=class_names)
        transformed_image = transformed['image']
        # also save the transformed image at the same time to avoid doing it again later
        cv2.imwrite(str(output_img_dir / filename), transformed_image)

        transformed_bboxes = np.array(transformed['bboxes'], dtype=np.int32)
        transformed_class_names = transformed['class_names']

        for bbox, class_name in zip(transformed_bboxes, transformed_class_names):
            value = (
                filename,
                width,
                height,
                class_name,
                *bbox
            )
            xml_list.append(value)

        col_names = [
            "filename",
            "width",
            "height",
            "classname",
            "xmin",
            "ymin",
            "xmax",
            "ymax",
        ]

    xml_df = pd.DataFrame(xml_list, columns=col_names)
    xml_df.to_csv(csv_path, index=False)
    time_elapsed = time.perf_counter() - start
    logger.info(f"Done. {time_elapsed = :.4f} seconds")


def load_labelmap(labelmap_path):
    """
    Returns:
    category_index =
    {
        1: {'id': 1, 'name': 'category_1'},
        2: {'id': 2, 'name': 'category_2'},
        3: {'id': 3, 'name': 'category_3'},
        ...
    }
    """
    category_index = label_map_util.create_category_index_from_labelmap(
        labelmap_path,
        use_display_name=True)
    return category_index


# ******************************* TFOD funcs *******************************

# ************************ Segmentation model funcs ************************

def load_mask_image(ori_image_name: str, mask_dir: Path) -> np.ndarray:
    """Given the `ori_image_name` (refers to the original non-mask image),
    parse the mask image filename and find and load it from the `mask_dir` folder."""
    mask_image_name = os.path.splitext(ori_image_name)[0] + ".png"
    mask_path = mask_dir / mask_image_name
    # MUST read in GRAYSCALE format to accurately preserve all the pixel values
    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
    return mask


@st.experimental_memo
def get_coco_classes(
    json_path: Union[str, Path],
    return_coco: bool = True) -> Union[Tuple[COCO, List[int], List[str]],
                                       List[str]]:
    """Get COCO classnames from the COCO JSON file.

    If `return_coco` is True, will return a tuple: (coco, catIDs, classnames)
    """
    coco = COCO(json_path)
    catIDs = coco.getCatIds()
    categories = coco.loadCats(catIDs)
    logger.debug(f"{categories = }")

    classnames = [cat["name"] for cat in categories]
    # add a background class at index 0
    classnames = ["background"] + classnames
    logger.debug(f"{classnames = }")

    if return_coco:
        return coco, catIDs, classnames

    return classnames


def generate_mask_images(coco_json_path: Union[str, Path] = None,
                         output_dir: Path = None,
                         n_masks: int = None,
                         verbose: bool = False,
                         st_container=None):
    """Generate mask images based on a COCO JSON file and save at `output_dir`

    Args:
        coco_json_path (Union[str, Path], optional): Path to the COCO JSON file.
            If None, infer from project export path. Defaults to None.
        output_dir (Path, optional): Path to output mask images.
            If None, infer from project export path. Defaults to None.
        n_masks (int, optional): Maximum number of mask images to generate,
            this is currently only used for the augmentation demo. If None, just 
            generate all mask images for all images in COCO JSON. Defaults to None.
        verbose (bool, optional): If True, will log the mask image info to console.
            Defaults to False.
        st_container ([type], optional): For `stqdm` progress bar. Can optionally pass
            in `st.sidebar`. Defaults to None.
    """
    data_export_dir = session_state.project.get_export_path()
    if not coco_json_path:
        coco_json_path = data_export_dir / "result.json"
    if not output_dir:
        output_dir = data_export_dir / "masks"
    os.makedirs(output_dir, exist_ok=True)

    json_file = json.load(open(coco_json_path))
    img_dict_list = json_file["images"]
    if n_masks:
        # optionally only generate certain number of mask images,
        # currently using this for the augmentation demo at the config page
        img_dict_list = img_dict_list[:n_masks]
    total_masks = len(img_dict_list)

    coco, catIDs, classnames = get_coco_classes(coco_json_path)

    logger.debug(f"Generating {total_masks} mask images in: {output_dir}")
    for img_dict in stqdm(img_dict_list, total=total_masks,
                          desc='Generating mask images', st_container=st_container):
        filename = os.path.basename(img_dict["file_name"])

        annIds = coco.getAnnIds(
            imgIds=img_dict["id"], catIds=catIDs, iscrowd=None)
        anns = coco.loadAnns(annIds)

        mask = np.zeros((img_dict["height"], img_dict["width"]))
        classes_found = []
        for annot in anns:
            # considering 'background' class at index 0
            className = classnames[annot["category_id"] + 1]
            classes_found.append(className)
            pixel_value = classnames.index(className)
            # the final mask contains the pixel values for each class
            mask = np.maximum(coco.annToMask(annot) * pixel_value, mask)

        # save the mask images in PNG format to preserve the exact pixel values
        mask_filename = os.path.splitext(filename)[0] + ".png"
        mask_path = os.path.join(output_dir, mask_filename)
        success = cv2.imwrite(mask_path, mask)
        if success:
            logger.debug(f"Generated mask image for {mask_filename}")

        if verbose:
            logger.debug(
                f"{filename} | "
                f"Unique pixel values = {np.unique(mask)} | "
                f"Unique classes found = {set(classes_found)} | "
                f"Number of annotations = {len(classes_found)}"  # or len(anns)
            )


def hybrid_loss(y_true, y_pred):
    loss_focal = focal_tversky(y_true, y_pred, alpha=0.5, gamma=4 / 3)
    loss_iou = iou_seg(y_true, y_pred)

    return loss_focal + loss_iou

# ************************ Segmentation model funcs ************************
